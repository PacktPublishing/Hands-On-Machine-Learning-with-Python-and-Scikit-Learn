{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart Disease\n",
    "\n",
    "This dataset contains 76 features, but all published experiments refer to using a subset of 14 of them. The \"goal\" feature refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4 (values 1,2,3,4) from absence (value 0). It is therefore a multiclass classification problem.\n",
    "\n",
    "*For our example, we will use several more features than the traditional 14.*\n",
    "\n",
    "Feature info (attributes used): \n",
    "1. feature 3 (age) - Age in years\n",
    "2. feature 4 (sex) - male or female\n",
    "3. feature 9 (cp) - chest pain type (typical angina, atypical angina, non-anginal, asymptomatic)\n",
    "4. feature 10 (trestbps) - resting blood pressure (mm Hg)\n",
    "5. feature 12 (chol) - cholesterol (mg/dl)\n",
    "6. feature 14 (cigperday) - number of cigarettes per day\n",
    "7. feature 16 (fbs) - fasting blood sugar > 120 mg/dl (1 = true; 0 = false) \n",
    "8. feature 18 (famhist) - family history of heart disease (1 = true; 0 = false)\n",
    "9. feature 19 (restecg) - resting electrocardiographic results (normal; st-t = having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV); vent = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n",
    "10. feature 32 (thalach) - maximum heart rate achieved\n",
    "11. feature 38 (exang) - exercise induced angina (1 = yes; 0 = no)\n",
    "12. feature 40 (oldpeak) - ST depression induced by exercise relative to rest\n",
    "13. feature 41 (slope) - the slope of the peak exercise ST segment (upsloping, flat, downsloping)\n",
    "14. feature 44 (ca) - number of major vessels (0-3) colored by flourosopy\n",
    "15. feature 51 (thal) - normal, fixed defect, or reversable defect\n",
    "16. feature 58 (target) (the predicted attribute) \n",
    "  - 0: < 50% diameter narrowing\n",
    "  - 1+: > 50% diameter narrowing\n",
    "\n",
    "### Our focus in using this dataset will be exploring pre-processing methods more thoroughly\n",
    "\n",
    "More details can be found at [the UCI repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease).\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "The authors of the dataset have requested that any use of the data include the names of the principal investigator responsible for the data collection at each institution. They would be: \n",
    "\n",
    "1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D. \n",
    "2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D. \n",
    "3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D. \n",
    "4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data from CSV\n",
    "\n",
    "We can read the data directly from the CSV located in the [data/](data/) directory. The [raw data](data/heart-disease-raw.csv) was pre-processed to re-name categorical features where they are otherwise ordinal variables. This allows us to walk through an entire pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>male</td>\n",
       "      <td>typical anginal</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>flat</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>flat</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>male</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>downsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>female</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>vent</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex                cp  trestbps  chol  cigperday  fbs  famhist  \\\n",
       "0   63    male   typical anginal       145   233       50.0    1        1   \n",
       "1   67    male      asymptomatic       160   286       40.0    0        1   \n",
       "2   67    male      asymptomatic       120   229       20.0    0        1   \n",
       "3   37    male       non-anginal       130   250        0.0    0        1   \n",
       "4   41  female  atypical anginal       130   204        0.0    0        1   \n",
       "\n",
       "  restecg  thalach  exang  oldpeak        slope   ca        thal  \n",
       "0    vent      150      0      2.3  downsloping  0.0       fixed  \n",
       "1    vent      108      1      1.5         flat  3.0      normal  \n",
       "2    vent      129      1      2.6         flat  2.0  reversable  \n",
       "3  normal      187      0      3.5  downsloping  0.0      normal  \n",
       "4    vent      172      0      1.4    upsloping  0.0      normal  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functions import cls as packt_classes\n",
    "\n",
    "# read the raw csv\n",
    "X = pd.read_csv('data/heart-disease-2.csv', header=None)\n",
    "\n",
    "# rename the columns\n",
    "cols = ['age', 'sex', 'cp', 'trestbps', 'chol', 'cigperday', 'fbs', 'famhist',\n",
    "        'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "\n",
    "X.columns = cols\n",
    "y = X.pop('target')  # don't want target in the X matrix\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-split: any major imbalance?\n",
    "\n",
    "If there are any categorical features with rare factor levels that need to be considered before splitting, we'll find out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "male      206\n",
      "female     97\n",
      "Name: sex, dtype: int64\n",
      "\n",
      "cp\n",
      "asymptomatic        144\n",
      "non-anginal          86\n",
      "atypical anginal     50\n",
      "typical anginal      23\n",
      "Name: cp, dtype: int64\n",
      "\n",
      "restecg\n",
      "normal    151\n",
      "vent      148\n",
      "st-t        4\n",
      "Name: restecg, dtype: int64\n",
      "\n",
      "slope\n",
      "upsloping      142\n",
      "flat           140\n",
      "downsloping     21\n",
      "Name: slope, dtype: int64\n",
      "\n",
      "thal\n",
      "normal        166\n",
      "reversable    117\n",
      "fixed          18\n",
      "Name: thal, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def examine_cats(frame):\n",
    "    for catcol in frame.columns[frame.dtypes == 'object'].tolist():\n",
    "        print(catcol)\n",
    "        print(frame[catcol].value_counts())\n",
    "        print(\"\")\n",
    "        \n",
    "examine_cats(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform train/test split\n",
    "\n",
    "Remember, we always need to split! We will also stratify on the '`restecg`' variable since it's the most likely to be poorly split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 227\n",
      "Test size: 76\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>54</td>\n",
       "      <td>female</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>132</td>\n",
       "      <td>288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>vent</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>52</td>\n",
       "      <td>male</td>\n",
       "      <td>non-anginal</td>\n",
       "      <td>138</td>\n",
       "      <td>223</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>male</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>flat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>63</td>\n",
       "      <td>female</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>2.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>52</td>\n",
       "      <td>male</td>\n",
       "      <td>atypical anginal</td>\n",
       "      <td>120</td>\n",
       "      <td>325</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>normal</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>upsloping</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age     sex                cp  trestbps  chol  cigperday  fbs  famhist  \\\n",
       "167   54  female  atypical anginal       132   288        0.0    1        0   \n",
       "166   52    male       non-anginal       138   223       50.0    0        1   \n",
       "300   57    male      asymptomatic       130   131       50.0    0        0   \n",
       "185   63  female  atypical anginal       140   195        2.0    0        1   \n",
       "84    52    male  atypical anginal       120   325       30.0    0        1   \n",
       "\n",
       "    restecg  thalach  exang  oldpeak      slope   ca        thal  \n",
       "167    vent      159      1      0.0  upsloping  1.0      normal  \n",
       "166  normal      169      0      0.0  upsloping  NaN      normal  \n",
       "300  normal      115      1      1.2       flat  1.0  reversable  \n",
       "185  normal      179      0      0.0  upsloping  2.0      normal  \n",
       "84   normal      172      0      0.2  upsloping  0.0      normal  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed, \n",
    "                                                    stratify=X['restecg'])\n",
    "\n",
    "print(\"Train size: %i\" % X_train.shape[0])\n",
    "print(\"Test size: %i\" % X_test.shape[0])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "male      153\n",
      "female     74\n",
      "Name: sex, dtype: int64\n",
      "\n",
      "cp\n",
      "asymptomatic        105\n",
      "non-anginal          66\n",
      "atypical anginal     37\n",
      "typical anginal      19\n",
      "Name: cp, dtype: int64\n",
      "\n",
      "restecg\n",
      "normal    113\n",
      "vent      111\n",
      "st-t        3\n",
      "Name: restecg, dtype: int64\n",
      "\n",
      "slope\n",
      "flat           110\n",
      "upsloping      102\n",
      "downsloping     15\n",
      "Name: slope, dtype: int64\n",
      "\n",
      "thal\n",
      "normal        121\n",
      "reversable     92\n",
      "fixed          12\n",
      "Name: thal, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examine_cats(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformers\n",
    "\n",
    "There are several custom transformers that will be useful for this data:\n",
    "\n",
    "- Custom one-hot encoding that drops one level to avoid the [dummy variable trap](http://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
    "- Model-based imputation of continuous variables, since mean/median centering is rudimentary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom base class\n",
    "\n",
    "We'll start with a cusom base class that depends on the input to be a Pandas dataframe. This base class will provide super methods for validating the input type as well as the presence of any prescribed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class CustomPandasTransformer(BaseEstimator, TransformerMixin):\n",
    "    def _validate_input(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X must be a DataFrame, but got type=%s\" \n",
    "                            % type(X))\n",
    "        return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def _validate_columns(X, cols):\n",
    "        scols = set(X.columns)  # set for O(1) lookup\n",
    "        if not all(c in scols for c in cols):\n",
    "            raise ValueError(\"all columns must be present in X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = ['banana', 'apple', 'orange', 'apple', 'orange']\n",
    "le = LabelEncoder()\n",
    "le.fit(labels)\n",
    "le.transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode categorical data\n",
    "\n",
    "It is probably (hopefully) obvious why we need to handle data that is in string format. There is not much we can do numerically with data that resembles the following:\n",
    "\n",
    "    [flat, upsloping, downsloping, ..., flat, flat, downsloping]\n",
    "    \n",
    "There is a natural procedure to force numericism amongst string data: map each unique string to a unique level (1, 2, 3). This is, in fact, exactly what the sklearn [`LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) does. However, this is not sufficient for modeling purposes, since most algorithms will treat this as [ordinal data](https://en.wikipedia.org/wiki/Ordinal_data), where in many cases it is not. Imagine you fit a regression on data you've label-encoded, and one feature (type of chest pain, for instance) is now:\n",
    "\n",
    "    [0, 2, 3, ..., 1, 0]\n",
    "    \n",
    "You might get coefficients back that make no sense since \"asymptomatic\" or \"non-anginal\", etc., are not inherently numerically greater or less than one another. Therefore, we [*one-hot encode*](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) our categorical data into a numerical representation. Now we have dummy data and a binary feature for each variable/factor-level combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "class DummyEncoder(CustomPandasTransformer):\n",
    "    \"\"\"A custom one-hot encoding class that handles previously unseen\n",
    "    levels and automatically drops one level from each categorical\n",
    "    feature to avoid the dummy variable trap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    columns : list\n",
    "        The list of columns that should be dummied\n",
    "        \n",
    "    sep : str or unicode, optional (default='_')\n",
    "        The string separator between the categorical feature name\n",
    "        and the level name.\n",
    "        \n",
    "    drop_one_level : bool, optional (default=True)\n",
    "        Whether to drop one level for each categorical variable.\n",
    "        This helps avoid the dummy variable trap.\n",
    "        \n",
    "    tmp_nan_rep : str or unicode, optional (default=\"N/A\")\n",
    "        Each categorical variable adds a level for missing values\n",
    "        so test data that is missing data will not break the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, columns, sep='_', drop_one_level=True, \n",
    "                 tmp_nan_rep='N/A'):\n",
    "        self.columns = columns\n",
    "        self.sep = sep\n",
    "        self.drop_one_level = drop_one_level\n",
    "        self.tmp_nan_rep = tmp_nan_rep\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # validate the input, and get a copy of it\n",
    "        X = self._validate_input(X).copy()\n",
    "        \n",
    "        # load class attributes into local scope\n",
    "        tmp_nan = self.tmp_nan_rep\n",
    "        \n",
    "        # validate all the columns present\n",
    "        cols = self.columns\n",
    "        self._validate_columns(X, cols)\n",
    "                \n",
    "        # begin fit\n",
    "        # for each column, fit a label encoder\n",
    "        lab_encoders = {}\n",
    "        for col in cols:\n",
    "            vec = [tmp_nan if pd.isnull(v) \n",
    "                   else v for v in X[col].tolist()]\n",
    "            \n",
    "            # if the tmp_nan value is not present in vec, make sure it is\n",
    "            # so the transform won't break down\n",
    "            svec = list(set(vec))\n",
    "            if tmp_nan not in svec:\n",
    "                svec.append(tmp_nan)\n",
    "            \n",
    "            le = LabelEncoder()\n",
    "            lab_encoders[col] = le.fit(svec)\n",
    "            \n",
    "            # transform the column, re-assign\n",
    "            X[col] = le.transform(vec)\n",
    "            \n",
    "        # fit a single OHE on the transformed columns - but we need to ensure\n",
    "        # the N/A tmp_nan vals make it into the OHE or it will break down later.\n",
    "        # this is a hack - add a row of all transformed nan levels\n",
    "        ohe_set = X[cols]\n",
    "        ohe_nan_row = {c: lab_encoders[c].transform([tmp_nan])[0] for c in cols}\n",
    "        ohe_set = ohe_set.append(ohe_nan_row, ignore_index=True)\n",
    "        ohe = OneHotEncoder(sparse=False).fit(ohe_set)\n",
    "        \n",
    "        # assign fit params\n",
    "        self.ohe_ = ohe\n",
    "        self.le_ = lab_encoders\n",
    "        self.cols_ = cols\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'ohe_')\n",
    "        X = self._validate_input(X).copy()\n",
    "        \n",
    "        # fit params that we need\n",
    "        ohe = self.ohe_\n",
    "        lenc = self.le_\n",
    "        cols = self.cols_\n",
    "        tmp_nan = self.tmp_nan_rep\n",
    "        sep = self.sep\n",
    "        drop = self.drop_one_level\n",
    "        \n",
    "        # validate the cols and the new X\n",
    "        self._validate_columns(X, cols)\n",
    "        col_order = []\n",
    "        drops = []\n",
    "        \n",
    "        for col in cols:\n",
    "            # get the vec from X, transform its nans if present\n",
    "            vec = [tmp_nan if pd.isnull(v) \n",
    "                   else v for v in X[col].tolist()]\n",
    "            \n",
    "            le = lenc[col]\n",
    "            vec_trans = le.transform(vec)  # str -> int\n",
    "            X[col] = vec_trans\n",
    "            \n",
    "            # get the column names (levels) so we can predict the \n",
    "            # order of the output cols\n",
    "            le_clz = le.classes_.tolist()\n",
    "            classes = [\"%s%s%s\" % (col, sep, clz) for clz in le_clz]\n",
    "            col_order.extend(classes)\n",
    "            \n",
    "            # if we want to drop one, just drop the last\n",
    "            if drop and len(le_clz) > 1:\n",
    "                drops.append(classes[-1])\n",
    "                \n",
    "        # now we can get the transformed OHE\n",
    "        ohe_trans = pd.DataFrame.from_records(data=ohe.transform(X[cols]), \n",
    "                                              columns=col_order)\n",
    "        \n",
    "        # set the index to be equal to X's for a smooth concat\n",
    "        ohe_trans.index = X.index\n",
    "        \n",
    "        # if we're dropping one level, do so now\n",
    "        if drops:\n",
    "            ohe_trans = ohe_trans.drop(drops, axis=1)\n",
    "        \n",
    "        # drop the original columns from X\n",
    "        X = X.drop(cols, axis=1)\n",
    "        \n",
    "        # concat the new columns\n",
    "        X = pd.concat([X, ohe_trans], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>cp_non-anginal</th>\n",
       "      <th>restecg_N/A</th>\n",
       "      <th>restecg_normal</th>\n",
       "      <th>restecg_st-t</th>\n",
       "      <th>slope_N/A</th>\n",
       "      <th>slope_downsloping</th>\n",
       "      <th>slope_flat</th>\n",
       "      <th>thal_N/A</th>\n",
       "      <th>thal_fixed</th>\n",
       "      <th>thal_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>54</td>\n",
       "      <td>132</td>\n",
       "      <td>288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>52</td>\n",
       "      <td>138</td>\n",
       "      <td>223</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>63</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>52</td>\n",
       "      <td>120</td>\n",
       "      <td>325</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  trestbps  chol  cigperday  fbs  famhist  thalach  exang  oldpeak  \\\n",
       "167   54       132   288        0.0    1        0      159      1      0.0   \n",
       "166   52       138   223       50.0    0        1      169      0      0.0   \n",
       "300   57       130   131       50.0    0        0      115      1      1.2   \n",
       "185   63       140   195        2.0    0        1      179      0      0.0   \n",
       "84    52       120   325       30.0    0        1      172      0      0.2   \n",
       "\n",
       "      ca     ...       cp_non-anginal  restecg_N/A  restecg_normal  \\\n",
       "167  1.0     ...                  0.0          0.0             0.0   \n",
       "166  NaN     ...                  1.0          0.0             1.0   \n",
       "300  1.0     ...                  0.0          0.0             1.0   \n",
       "185  2.0     ...                  0.0          0.0             1.0   \n",
       "84   0.0     ...                  0.0          0.0             1.0   \n",
       "\n",
       "     restecg_st-t  slope_N/A  slope_downsloping  slope_flat  thal_N/A  \\\n",
       "167           0.0        0.0                0.0         0.0       0.0   \n",
       "166           0.0        0.0                0.0         0.0       0.0   \n",
       "300           0.0        0.0                0.0         1.0       0.0   \n",
       "185           0.0        0.0                0.0         0.0       0.0   \n",
       "84            0.0        0.0                0.0         0.0       0.0   \n",
       "\n",
       "     thal_fixed  thal_normal  \n",
       "167         0.0          1.0  \n",
       "166         0.0          1.0  \n",
       "300         0.0          0.0  \n",
       "185         0.0          1.0  \n",
       "84          0.0          1.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de = DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])\n",
    "de.fit(X_train)\n",
    "X_train_dummied = de.transform(X_train)\n",
    "X_train_dummied.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "We can either use a built-in scikit-learn `Imputer`, which will require mean/median as a statistic, or we can build a model.\n",
    "\n",
    "## Statistic-based imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.40000000e+01,   1.32000000e+02,   2.88000000e+02,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.59000000e+02,   1.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00],\n",
       "       [  5.20000000e+01,   1.38000000e+02,   2.23000000e+02,\n",
       "          5.00000000e+01,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.69000000e+02,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00],\n",
       "       [  5.70000000e+01,   1.30000000e+02,   1.31000000e+02,\n",
       "          5.00000000e+01,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.15000000e+02,   1.00000000e+00,   1.20000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   1.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "       [  6.30000000e+01,   1.40000000e+02,   1.95000000e+02,\n",
       "          2.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.79000000e+02,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00],\n",
       "       [  5.20000000e+01,   1.20000000e+02,   3.25000000e+02,\n",
       "          3.00000000e+01,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.72000000e+02,   0.00000000e+00,   2.00000000e-01,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          1.00000000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy='median')\n",
    "imputer.fit(X_train_dummied)\n",
    "imputer.transform(X_train_dummied)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model-based imputation\n",
    "\n",
    "As discussed in the iris notebook, there are many pitfalls to using mean or median for scaling. In instances where our data is too large to examine all features graphically, many times we cannot discern whether all features are normally distributed (a pre-requisite for mean-scaling). If we want to get more sophisticated, we can use an approach for imputation that is based on a model; we will use a [`BaggingRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) (since we are filling in NaN continuous variables only at this point).\n",
    "\n",
    "Note that there are other common approaches for this, like KNN imputation, but nearest neighbors models require your data to be scaled, which we're trying to avoid.\n",
    "\n",
    "### Beware:\n",
    "\n",
    "Sometimes missing data is informative. For instance... failure to report `cigperday` could be a bias on part of the patient who may not want to receive judgment or a lecture, or could indicate 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.externals import six\n",
    "\n",
    "class BaggedRegressorImputer(CustomPandasTransformer):\n",
    "    \"\"\"Fit bagged regressor models for each of the impute columns in order\n",
    "    to impute the missing values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    impute_cols : list\n",
    "        The columns to impute\n",
    "        \n",
    "    base_estimator : object or None, optional (default=None)\n",
    "        The base estimator to fit on random subsets of the dataset.\n",
    "        If None, then the base estimator is a decision tree.\n",
    "\n",
    "    n_estimators : int, optional (default=10)\n",
    "        The number of base estimators in the ensemble.\n",
    "\n",
    "    max_samples : int or float, optional (default=1.0)\n",
    "        The number of samples to draw from X to train each base estimator.\n",
    "            - If int, then draw `max_samples` samples.\n",
    "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "\n",
    "    max_features : int or float, optional (default=1.0)\n",
    "        The number of features to draw from X to train each base estimator.\n",
    "            - If int, then draw `max_features` features.\n",
    "            - If float, then draw `max_features * X.shape[1]` features.\n",
    "\n",
    "    bootstrap : boolean, optional (default=True)\n",
    "        Whether samples are drawn with replacement.\n",
    "\n",
    "    bootstrap_features : boolean, optional (default=False)\n",
    "        Whether features are drawn with replacement.\n",
    "        \n",
    "    n_jobs : int, optional (default=1)\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        If -1, then the number of jobs is set to the number of cores.\n",
    "\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    verbose : int, optional (default=0)\n",
    "        Controls the verbosity of the building process.\n",
    "    \"\"\"\n",
    "    def __init__(self, impute_cols, base_estimator=None, n_estimators=10, \n",
    "                 max_samples=1.0, max_features=1.0, bootstrap=True, \n",
    "                 bootstrap_features=False, n_jobs=1,\n",
    "                 random_state=None, verbose=0):\n",
    "        \n",
    "        self.impute_cols = impute_cols\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.bootstrap_features = bootstrap_features\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # validate that the input is a dataframe\n",
    "        X = self._validate_input(X)  # don't need a copy this time\n",
    "        \n",
    "        # validate the columns exist in the dataframe\n",
    "        cols = self.impute_cols\n",
    "        self._validate_columns(X, cols)\n",
    "        \n",
    "        # this dictionary will hold the models\n",
    "        regressors = {}\n",
    "        \n",
    "        # this dictionary maps the impute column name(s) to the vecs\n",
    "        targets = {c: X[c] for c in cols}\n",
    "        \n",
    "        # drop off the columns we'll be imputing as targets\n",
    "        X = X.drop(cols, axis=1)  # these should all be filled in (no NaN)\n",
    "        \n",
    "        # iterate the column names and the target columns\n",
    "        for k, target in six.iteritems(targets):\n",
    "            # split X row-wise into train/test where test is the missing\n",
    "            # rows in the target\n",
    "            test_mask = pd.isnull(target)\n",
    "            train = X.loc[~test_mask]\n",
    "            train_y = target[~test_mask]\n",
    "            \n",
    "            # fit the regressor\n",
    "            regressors[k] = BaggingRegressor(\n",
    "                base_estimator=self.base_estimator,\n",
    "                n_estimators=self.n_estimators,\n",
    "                max_samples=self.max_samples,\n",
    "                max_features=self.max_features,\n",
    "                bootstrap=self.bootstrap,\n",
    "                bootstrap_features=self.bootstrap_features,\n",
    "                n_jobs=self.n_jobs, \n",
    "                random_state=self.random_state,\n",
    "                verbose=self.verbose, oob_score=False,\n",
    "                warm_start=False).fit(train, train_y)\n",
    "            \n",
    "        # assign fit params\n",
    "        self.regressors_ = regressors\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, 'regressors_')\n",
    "        X = self._validate_input(X).copy()  # need a copy\n",
    "        \n",
    "        cols = self.impute_cols\n",
    "        self._validate_columns(X, cols)\n",
    "        \n",
    "        # fill in the missing\n",
    "        models = self.regressors_\n",
    "        for k, model in six.iteritems(models):\n",
    "            target = X[k]\n",
    "            \n",
    "            # split X row-wise into train/test where test is the missing\n",
    "            # rows in the target\n",
    "            test_mask = pd.isnull(target)\n",
    "            \n",
    "            # if there's nothing missing in the test set for this feature, skip\n",
    "            if test_mask.sum() == 0:\n",
    "                continue\n",
    "            test = X.loc[test_mask].drop(cols, axis=1)  # drop impute cols\n",
    "            \n",
    "            # generate predictions\n",
    "            preds = model.predict(test)\n",
    "            \n",
    "            # impute!\n",
    "            X.loc[test_mask, k] = preds\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>cigperday</th>\n",
       "      <th>fbs</th>\n",
       "      <th>famhist</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>...</th>\n",
       "      <th>cp_non-anginal</th>\n",
       "      <th>restecg_N/A</th>\n",
       "      <th>restecg_normal</th>\n",
       "      <th>restecg_st-t</th>\n",
       "      <th>slope_N/A</th>\n",
       "      <th>slope_downsloping</th>\n",
       "      <th>slope_flat</th>\n",
       "      <th>thal_N/A</th>\n",
       "      <th>thal_fixed</th>\n",
       "      <th>thal_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>54</td>\n",
       "      <td>132</td>\n",
       "      <td>288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>52</td>\n",
       "      <td>138</td>\n",
       "      <td>223</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>63</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>52</td>\n",
       "      <td>120</td>\n",
       "      <td>325</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  trestbps  chol  cigperday  fbs  famhist  thalach  exang  oldpeak  \\\n",
       "167   54       132   288        0.0    1        0      159      1      0.0   \n",
       "166   52       138   223       50.0    0        1      169      0      0.0   \n",
       "300   57       130   131       50.0    0        0      115      1      1.2   \n",
       "185   63       140   195        2.0    0        1      179      0      0.0   \n",
       "84    52       120   325       30.0    0        1      172      0      0.2   \n",
       "\n",
       "      ca     ...       cp_non-anginal  restecg_N/A  restecg_normal  \\\n",
       "167  1.0     ...                  0.0          0.0             0.0   \n",
       "166  0.1     ...                  1.0          0.0             1.0   \n",
       "300  1.0     ...                  0.0          0.0             1.0   \n",
       "185  2.0     ...                  0.0          0.0             1.0   \n",
       "84   0.0     ...                  0.0          0.0             1.0   \n",
       "\n",
       "     restecg_st-t  slope_N/A  slope_downsloping  slope_flat  thal_N/A  \\\n",
       "167           0.0        0.0                0.0         0.0       0.0   \n",
       "166           0.0        0.0                0.0         0.0       0.0   \n",
       "300           0.0        0.0                0.0         1.0       0.0   \n",
       "185           0.0        0.0                0.0         0.0       0.0   \n",
       "84            0.0        0.0                0.0         0.0       0.0   \n",
       "\n",
       "     thal_fixed  thal_normal  \n",
       "167         0.0          1.0  \n",
       "166         0.0          1.0  \n",
       "300         0.0          0.0  \n",
       "185         0.0          1.0  \n",
       "84          0.0          1.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_imputer = BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], \n",
    "                                        random_state=seed)\n",
    "bagged_imputer.fit(X_train_dummied)\n",
    "\n",
    "# save the masks so we can look at them afterwards\n",
    "ca_nan_mask = pd.isnull(X_train_dummied.ca)\n",
    "cpd_nan_mask = pd.isnull(X_train_dummied.cigperday)\n",
    "\n",
    "# impute\n",
    "X_train_imputed = bagged_imputer.transform(X_train_dummied)\n",
    "X_train_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166    0.1\n",
       "287    0.1\n",
       "Name: ca, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imputed[ca_nan_mask].ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195    18.6\n",
       "21      2.0\n",
       "Name: cigperday, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imputed[cpd_nan_mask].cigperday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_imputed.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection/dimensionality reduction\n",
    "\n",
    "Often times, when there is very high-dimensional data (100s or 1000s of features), it's useful to perform feature selection techniques to create more simple models that can be understood by analysts. A common one is [principal components analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), but one of its drawbacks is diminished model clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_imputed)\n",
    "\n",
    "# fit PCA, get explained variance of ALL features\n",
    "pca_all = PCA(n_components=None)\n",
    "pca_all.fit(scaler.transform(X_train_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16495945,  0.26044635,  0.33313074,  0.40270312,  0.46565191,\n",
       "        0.52339203,  0.57474356,  0.6224878 ,  0.66791793,  0.71144846,\n",
       "        0.75296637,  0.79143841,  0.82880222,  0.86117486,  0.89239854,\n",
       "        0.9206686 ,  0.9445678 ,  0.96357808,  0.98196293,  0.99471478,\n",
       "        1.        ,  1.        ,  1.        ,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_var = np.cumsum(pca_all.explained_variance_ratio_)\n",
    "explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVoD7dFDRjcJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1417d5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative explained variance at 15 components: 0.92067\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x_axis = np.arange(X_train_imputed.shape[1]) + 1\n",
    "plt.plot(x_axis, explained_var)\n",
    "\n",
    "# At which point to cut off?\n",
    "minexp = np.where(explained_var > 0.9)[0][0]\n",
    "plt.axvline(x=minexp, linestyle='dashed', color='red', alpha=0.5)\n",
    "plt.xticks(x_axis)\n",
    "plt.show()\n",
    "\n",
    "print(\"Cumulative explained variance at %i components: %.5f\" % (minexp, explained_var[minexp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 15 (of 25) features, we finally explain >90% cumulative variance in our components. This is not a significant enough feature reduction to warrant use of PCA, so we'll skip it.\n",
    "\n",
    "# Setup our CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# set up our CV\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Training sample indices:\n",
      "array([  1,   3,   4,   5,   6,   8,   9,  10,  12,  13,  15,  17,  18,\n",
      "        21,  23,  24,  25,  26,  27,  28,  30,  31,  32,  34,  35,  36,\n",
      "        37,  38,  39,  41,  42,  43,  44,  45,  47,  49,  51,  53,  54,\n",
      "        55,  56,  58,  59,  62,  65,  66,  69,  70,  71,  73,  74,  75,\n",
      "        79,  81,  82,  85,  87,  89,  90,  93,  94,  95,  96,  97,  99,\n",
      "       100, 102, 103, 105, 106, 107, 108, 110, 111, 114, 115, 116, 117,\n",
      "       118, 119, 121, 123, 125, 127, 129, 131, 132, 134, 135, 136, 139,\n",
      "       140, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 156,\n",
      "       157, 158, 160, 162, 163, 166, 169, 170, 171, 172, 173, 174, 177,\n",
      "       180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 191, 193, 194,\n",
      "       198, 199, 200, 201, 203, 204, 205, 207, 209, 210, 211, 213, 215,\n",
      "       216, 218, 219, 220, 221, 224, 226])\n",
      "Testing sample indices:\n",
      "array([  0,   2,   7,  11,  14,  16,  19,  20,  22,  29,  33,  40,  46,\n",
      "        48,  50,  52,  57,  60,  61,  63,  64,  67,  68,  72,  76,  77,\n",
      "        78,  80,  83,  84,  86,  88,  91,  92,  98, 101, 104, 109, 112,\n",
      "       113, 120, 122, 124, 126, 128, 130, 133, 137, 138, 141, 142, 144,\n",
      "       155, 159, 161, 164, 165, 167, 168, 175, 176, 178, 179, 182, 192,\n",
      "       195, 196, 197, 202, 206, 208, 212, 214, 217, 222, 223, 225])\n",
      "\n",
      "\n",
      "Fold 1:\n",
      "Training sample indices:\n",
      "array([  0,   1,   2,   3,   7,  11,  14,  16,  19,  20,  22,  23,  26,\n",
      "        29,  30,  33,  35,  36,  39,  40,  43,  46,  47,  48,  49,  50,\n",
      "        52,  53,  54,  57,  59,  60,  61,  62,  63,  64,  65,  66,  67,\n",
      "        68,  69,  70,  72,  76,  77,  78,  79,  80,  81,  83,  84,  86,\n",
      "        87,  88,  91,  92,  93,  96,  98, 100, 101, 102, 103, 104, 108,\n",
      "       109, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123,\n",
      "       124, 126, 128, 130, 131, 132, 133, 134, 135, 137, 138, 139, 140,\n",
      "       141, 142, 143, 144, 145, 147, 153, 155, 156, 159, 161, 162, 163,\n",
      "       164, 165, 166, 167, 168, 171, 175, 176, 177, 178, 179, 180, 181,\n",
      "       182, 183, 185, 186, 189, 190, 191, 192, 194, 195, 196, 197, 198,\n",
      "       200, 201, 202, 206, 208, 209, 210, 211, 212, 213, 214, 217, 218,\n",
      "       219, 220, 221, 222, 223, 224, 225, 226])\n",
      "Testing sample indices:\n",
      "array([  4,   5,   6,   8,   9,  10,  12,  13,  15,  17,  18,  21,  24,\n",
      "        25,  27,  28,  31,  32,  34,  37,  38,  41,  42,  44,  45,  51,\n",
      "        55,  56,  58,  71,  73,  74,  75,  82,  85,  89,  90,  94,  95,\n",
      "        97,  99, 105, 106, 107, 110, 116, 125, 127, 129, 136, 146, 148,\n",
      "       149, 150, 151, 152, 154, 157, 158, 160, 169, 170, 172, 173, 174,\n",
      "       184, 187, 188, 193, 199, 203, 204, 205, 207, 215, 216])\n",
      "\n",
      "\n",
      "Fold 2:\n",
      "Training sample indices:\n",
      "array([  0,   2,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "        15,  16,  17,  18,  19,  20,  21,  22,  24,  25,  27,  28,  29,\n",
      "        31,  32,  33,  34,  37,  38,  40,  41,  42,  44,  45,  46,  48,\n",
      "        50,  51,  52,  55,  56,  57,  58,  60,  61,  63,  64,  67,  68,\n",
      "        71,  72,  73,  74,  75,  76,  77,  78,  80,  82,  83,  84,  85,\n",
      "        86,  88,  89,  90,  91,  92,  94,  95,  97,  98,  99, 101, 104,\n",
      "       105, 106, 107, 109, 110, 112, 113, 116, 120, 122, 124, 125, 126,\n",
      "       127, 128, 129, 130, 133, 136, 137, 138, 141, 142, 144, 146, 148,\n",
      "       149, 150, 151, 152, 154, 155, 157, 158, 159, 160, 161, 164, 165,\n",
      "       167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 182, 184,\n",
      "       187, 188, 192, 193, 195, 196, 197, 199, 202, 203, 204, 205, 206,\n",
      "       207, 208, 212, 214, 215, 216, 217, 222, 223, 225])\n",
      "Testing sample indices:\n",
      "array([  1,   3,  23,  26,  30,  35,  36,  39,  43,  47,  49,  53,  54,\n",
      "        59,  62,  65,  66,  69,  70,  79,  81,  87,  93,  96, 100, 102,\n",
      "       103, 108, 111, 114, 115, 117, 118, 119, 121, 123, 131, 132, 134,\n",
      "       135, 139, 140, 143, 145, 147, 153, 156, 162, 163, 166, 171, 177,\n",
      "       180, 181, 183, 185, 186, 189, 190, 191, 194, 198, 200, 201, 209,\n",
      "       210, 211, 213, 218, 219, 220, 221, 224, 226])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "folds = cv.split(X_train, y_train)\n",
    "for i, fold in enumerate(folds):\n",
    "    tr, te = fold\n",
    "    print(\"Fold %i:\" % i)\n",
    "    print(\"Training sample indices:\\n%r\" % tr)\n",
    "    print(\"Testing sample indices:\\n%r\" % te)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Baseline several models\n",
    "\n",
    "We will build three models with default parameters and look at how the cross validation scores perform across folds, then we'll select the two better models to take into the model tuning stage.\n",
    "\n",
    "__NOTE__ we could theoretically go straight to tuning all three models to select the best, but it is often times not feasible to run grid searches for every model you want to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# these are the pre-processing stages\n",
    "stages = [\n",
    "    ('dummy', packt_classes.DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])),\n",
    "    ('impute', packt_classes.BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], \n",
    "                                                    random_state=seed))\n",
    "]\n",
    "\n",
    "# we'll add a new estimator onto the end of the pre-processing stages\n",
    "def build_pipeline(pipe_stages, estimator, est_name='clf'):\n",
    "    # copy the stages\n",
    "    pipe_stages = [stage for stage in pipe_stages]\n",
    "    pipe_stages.append((est_name, estimator))\n",
    "    \n",
    "    # return the pipe\n",
    "    return Pipeline(pipe_stages)\n",
    "\n",
    "# report how the model did\n",
    "def cv_report(cv_scores):\n",
    "    mean = np.average(cv_scores)\n",
    "    std = np.std(cv_scores)\n",
    "    \n",
    "    print(\"CV scores: %r\" % cv_scores)\n",
    "    print(\"Average CV score: %.4f\" % mean)\n",
    "    print(\"CV score standard deviation: %.4f\" % std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: array([-1.25219065, -1.13973284, -1.06839339])\n",
      "Average CV score: -1.1534\n",
      "CV score standard deviation: 0.0757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# fit a Logistic regression\n",
    "lgr_pipe = build_pipeline(stages, LogisticRegression(random_state=seed))\n",
    "cv_report(cross_val_score(lgr_pipe, X=X_train, y=y_train, \n",
    "                          scoring='neg_log_loss', cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: array([-1.14662777, -1.19505187, -1.31838939])\n",
      "Average CV score: -1.2200\n",
      "CV score standard deviation: 0.0723\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# fit a GBM\n",
    "gbm_pipe = build_pipeline(stages, GradientBoostingClassifier(n_estimators=25, max_depth=3, random_state=seed))\n",
    "cv_report(cross_val_score(gbm_pipe, X=X_train, y=y_train, \n",
    "                          scoring='neg_log_loss', cv=cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: array([-1.09616462, -2.26438127, -1.94406386])\n",
      "Average CV score: -1.7682\n",
      "CV score standard deviation: 0.4929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# fit a RF\n",
    "rf_pipe = build_pipeline(stages, RandomForestClassifier(n_estimators=25, random_state=seed))\n",
    "cv_report(cross_val_score(rf_pipe, X=X_train, y=y_train, \n",
    "                          scoring='neg_log_loss', cv=cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial thoughts\n",
    "\n",
    "* Our GBM and logistic regression perform similarly\n",
    "* Random forest did not perform very well and showed high variability across training folds\n",
    "* Let's move forward with LR & GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyper-params\n",
    "\n",
    "Now that we've baselined several models, let's choose a couple of the better-performing models to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   55.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=True),\n",
       "          error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('dummy', DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'],\n",
       "       drop_one_level=True, sep='_', tmp_nan_rep='N/A')), ('impute', BaggedRegressorImputer(base_estimator=None, bootstrap=True,\n",
       "            bootstrap_features=False, impute_cols=['cigperday', 'ca'],\n",
       "            max_fea...        presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
       "              warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'clf__min_samples_split': [2, 4, 5, 10], 'clf__max_depth': [1, 3, 4, 5, 7], 'impute__n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a147d2f28>, 'clf__learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a147ff208>, 'clf__n_estimator...y.stats._distn_infrastructure.rv_frozen object at 0x1a147d2eb8>, 'clf__min_samples_leaf': [1, 2, 5]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=False, scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "gbm_pipe = Pipeline([\n",
    "    ('dummy', packt_classes.DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])),\n",
    "    ('impute', packt_classes.BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], \n",
    "                                                    random_state=seed)),\n",
    "    ('clf', GradientBoostingClassifier(random_state=seed))\n",
    "])\n",
    "\n",
    "# define the hyper-params\n",
    "hyper_params = {\n",
    "    'impute__n_estimators': randint(10, 50),\n",
    "    'impute__max_samples': uniform(0.75, 0.125),\n",
    "    'impute__max_features': uniform(0.75, 0.125),\n",
    "    'clf__n_estimators': randint(50, 400),\n",
    "    'clf__max_depth': [1, 3, 4, 5, 7],\n",
    "    'clf__learning_rate': uniform(0.05, 0.1),\n",
    "    'clf__min_samples_split': [2, 4, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# define the search\n",
    "gbm_search = RandomizedSearchCV(gbm_pipe, param_distributions=hyper_params,\n",
    "                                random_state=seed, cv=cv, n_iter=100,\n",
    "                                n_jobs=-1, verbose=1, scoring='neg_log_loss',\n",
    "                                return_train_score=False)\n",
    "\n",
    "gbm_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:    9.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=3, random_state=42, shuffle=True),\n",
       "          error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('dummy', DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'],\n",
       "       drop_one_level=True, sep='_', tmp_nan_rep='N/A')), ('impute', BaggedRegressorImputer(base_estimator=None, bootstrap=True,\n",
       "            bootstrap_features=False, impute_cols=['cigperday', 'ca'],\n",
       "            max_fea...alty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'impute__max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a14705978>, 'clf__max_iter': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a14705ba8>, 'clf__penalty': ['l1', 'l2'], 'impute__n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a14711b38>, 'clf__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a14705898>, 'impute__max_samples': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a147117f0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=False, scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr_pipe = Pipeline([\n",
    "    ('dummy', packt_classes.DummyEncoder(columns=['sex', 'cp', 'restecg', 'slope', 'thal'])),\n",
    "    ('impute', packt_classes.BaggedRegressorImputer(impute_cols=['cigperday', 'ca'], \n",
    "                                                    random_state=seed)),\n",
    "    ('clf', LogisticRegression(random_state=seed))\n",
    "])\n",
    "\n",
    "# define the hyper-params\n",
    "hyper_params = {\n",
    "    'impute__n_estimators': randint(10, 50),\n",
    "    'impute__max_samples': uniform(0.75, 0.125),\n",
    "    'impute__max_features': uniform(0.75, 0.125),\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': uniform(0.5, 0.125),\n",
    "    'clf__max_iter': randint(100, 500)\n",
    "}\n",
    "\n",
    "# define the search\n",
    "lgr_search = RandomizedSearchCV(lgr_pipe, param_distributions=hyper_params,\n",
    "                                random_state=seed, cv=cv, n_iter=100,\n",
    "                                n_jobs=-1, verbose=1, scoring='neg_log_loss',\n",
    "                                return_train_score=False)\n",
    "\n",
    "lgr_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine the results\n",
    "\n",
    "Right away we can tell that the logistic regression model was *much* faster than the gradient boosting model. However, does the extra time spent fitting end up giving us a performance boost? Let's introduce our test set to the optimized models and select the one that performs better. We are using [__log loss__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html) as a scoring metric.\n",
    "\n",
    "See [this answer](https://stats.stackexchange.com/questions/208443/intuitive-explanation-of-logloss) for a full intuitive explanation of log loss, but note that lower (closer to zero) is better. There is no maximum to log loss, and typically, the more classes you have, the higher it will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First the CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GBM</th>\n",
       "      <th>Log. Reg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_test_score_MEAN</th>\n",
       "      <td>-2.073543</td>\n",
       "      <td>-1.125759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score_STD</th>\n",
       "      <td>0.503357</td>\n",
       "      <td>0.002204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score_STD_OVER_FOLDS</th>\n",
       "      <td>0.357103</td>\n",
       "      <td>0.001514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     GBM  Log. Reg\n",
       "mean_test_score_MEAN           -2.073543 -1.125759\n",
       "mean_test_score_STD             0.503357  0.002204\n",
       "mean_test_score_STD_OVER_FOLDS  0.357103  0.001514"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import gen_batches\n",
    "\n",
    "def grid_report(search, n_splits, key='mean_test_score'):\n",
    "    res = search.cv_results_\n",
    "    arr = res[key]\n",
    "    slices = gen_batches(arr.shape[0], n_splits)\n",
    "    \n",
    "    return pd.Series({\n",
    "            '%s_MEAN' % key: arr.mean(),\n",
    "            '%s_STD' % key: arr.std(),\n",
    "             \n",
    "             # the std of fold scores for each set of hyper-params,\n",
    "             # averaged over all sets of params\n",
    "             '%s_STD_OVER_FOLDS' % key: np.asarray([\n",
    "                    arr[slc].std()\n",
    "                    for slc in slices\n",
    "                ]).mean()})\n",
    "    \n",
    "pd.DataFrame.from_records([grid_report(gbm_search, cv.get_n_splits()), \n",
    "                           grid_report(lgr_search, cv.get_n_splits())],\n",
    "                          index=[\"GBM\", \"Log. Reg\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the CV scores meet business requirements, move on to model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM test LOSS: 0.96101\n",
      "Logistic regression test LOSS: 0.97445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "gbm_preds = gbm_search.predict_proba(X_test)\n",
    "lgr_preds = lgr_search.predict_proba(X_test)\n",
    "\n",
    "print(\"GBM test LOSS: %.5f\" % log_loss(y_true=y_test, y_pred=gbm_preds))\n",
    "print(\"Logistic regression test LOSS: %.5f\" % log_loss(y_true=y_test, y_pred=lgr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that in log loss, greater is WORSE. Therefore, the logistic regression was out-performed by the GBM. If the greater time to fit is not an issue for you, then this would be the better model to select. Likewise, you may favor model transparency over the extra few decimal points of accuracy, in which case the logistic regression might be favorable.\n",
    "\n",
    "# Variable importance\n",
    "\n",
    "Most times, it's not enough to build a good model. Most executives will want to know *why* something works. Moreover, in regulated industries like banking or insurance, knowing why a model is working is incredibly important for defending models to a regulatory board.\n",
    "\n",
    "One of the methods commonly used for observing variable importance for non-linear methods (like our gradient boosting model) is to break the model into piecewise linear functions and measure how the model performs against each variable. This is called a \"partial dependency plot.\"\n",
    "\n",
    "### Raw feature importances\n",
    "\n",
    "We can get the raw feature importances from the estimator itself, and match them up with the transformed column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'trestbps', 'chol', 'cigperday', 'fbs', 'famhist', 'thalach',\n",
       "       'exang', 'oldpeak', 'ca', 'sex_N/A', 'sex_female', 'cp_N/A',\n",
       "       'cp_asymptomatic', 'cp_atypical anginal', 'cp_non-anginal',\n",
       "       'restecg_N/A', 'restecg_normal', 'restecg_st-t', 'slope_N/A',\n",
       "       'slope_downsloping', 'slope_flat', 'thal_N/A', 'thal_fixed',\n",
       "       'thal_normal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed data through the pipe stages to get the transformed feature names\n",
    "X_trans = X_train\n",
    "for step in gbm_search.best_estimator_.steps[:-1]:\n",
    "    X_trans = step[1].transform(X_trans)\n",
    "    \n",
    "transformed_feature_names = X_trans.columns\n",
    "transformed_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10196078,  0.08039216,  0.15490196,  0.02941176,  0.02352941,\n",
       "        0.0254902 ,  0.09607843,  0.01960784,  0.14313725,  0.13137255,\n",
       "        0.        ,  0.03921569,  0.        ,  0.04509804,  0.        ,\n",
       "        0.        ,  0.        ,  0.02745098,  0.        ,  0.        ,\n",
       "        0.        ,  0.0254902 ,  0.        ,  0.03137255,  0.0254902 ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gbm = gbm_search.best_estimator_.steps[-1][1]\n",
    "importances = best_gbm.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 'chol', 0.15490196078431367),\n",
       " (8, 'oldpeak', 0.14313725490196069),\n",
       " (9, 'ca', 0.13137254901960776),\n",
       " (0, 'age', 0.10196078431372549),\n",
       " (6, 'thalach', 0.096078431372549039),\n",
       " (1, 'trestbps', 0.080392156862745132),\n",
       " (13, 'cp_asymptomatic', 0.045098039215686295),\n",
       " (11, 'sex_female', 0.03921568627450981),\n",
       " (23, 'thal_fixed', 0.03137254901960785),\n",
       " (3, 'cigperday', 0.029411764705882356),\n",
       " (17, 'restecg_normal', 0.027450980392156866),\n",
       " (5, 'famhist', 0.025490196078431372),\n",
       " (21, 'slope_flat', 0.025490196078431372),\n",
       " (24, 'thal_normal', 0.025490196078431372),\n",
       " (4, 'fbs', 0.023529411764705882),\n",
       " (7, 'exang', 0.019607843137254898),\n",
       " (10, 'sex_N/A', 0.0),\n",
       " (12, 'cp_N/A', 0.0),\n",
       " (14, 'cp_atypical anginal', 0.0),\n",
       " (15, 'cp_non-anginal', 0.0),\n",
       " (16, 'restecg_N/A', 0.0),\n",
       " (18, 'restecg_st-t', 0.0),\n",
       " (19, 'slope_N/A', 0.0),\n",
       " (20, 'slope_downsloping', 0.0),\n",
       " (22, 'thal_N/A', 0.0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = sorted(zip(np.arange(len(transformed_feature_names)), \n",
    "                                 transformed_feature_names, \n",
    "                                 importances), \n",
    "                             key=(lambda ici: ici[2]),\n",
    "                             reverse=True)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial dependency\n",
    "\n",
    "In the following section, we'll break our GBM into a piecewise linear functions to gauge how different variables impact the target, and create [partial dependency plots](http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbMAAAEcCAYAAABNp1q8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104d8d400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "\n",
    "def plot_partial(est, which_features, X, names, label):\n",
    "    fig, axs = plot_partial_dependence(est, X, which_features,\n",
    "                                       feature_names=names,\n",
    "                                       n_jobs=3, grid_resolution=50,\n",
    "                                       label=label)\n",
    "    \n",
    "    fig.suptitle('Partial dependence of %i features\\n'\n",
    "                 'on heart disease' % (len(which_features)))\n",
    "    plt.subplots_adjust(top=0.8)  # tight_layout causes overlap with suptitle\n",
    "        \n",
    "plot_partial(est=best_gbm, X=X_trans,\n",
    "             which_features=[2, 8, 9, 0, 6, (2, 9)],\n",
    "             names=transformed_feature_names,\n",
    "             label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing\n",
    "\n",
    "Suppose our board of surgeons only cares if the prediction is class \"3\" with a probability of >=0.3. In this segment we'll write and test a piece of code that we'll use as post-processing in our Flask API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_certain_class(predictions, cls=3, proba=0.3):\n",
    "    # find the row arg maxes (ones that are predicted 'cls')\n",
    "    argmaxes = predictions.argmax(axis=1)\n",
    "    \n",
    "    # get the probas for the cls of interest\n",
    "    probas = predictions[:, cls]\n",
    "    \n",
    "    # boolean mask that becomes our prediction vector\n",
    "    return ((argmaxes == cls) & (probas >= proba)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we'll need to use \"`predict_proba`\" rather than \"`predict`\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06079324,  0.06840017,  0.41105351,  0.36934499,  0.09040809],\n",
       "       [ 0.75965849,  0.17536797,  0.05236385,  0.01112373,  0.00148597],\n",
       "       [ 0.54363485,  0.25425213,  0.15415816,  0.03354877,  0.01440608],\n",
       "       [ 0.14354267,  0.17970444,  0.57161936,  0.07778423,  0.02734929],\n",
       "       [ 0.91451301,  0.06250888,  0.00835557,  0.00902481,  0.00559774]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = lgr_search.predict_proba(X_test)\n",
    "P[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_certain_class(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn makes model persistence extraordinarily easily. Everything can be pickled via the \"joblib\" submodule. There are some exceptions:\n",
    "\n",
    "1. Classes that contain unbound methods\n",
    "2. Classes that contain instances of loggers\n",
    "3. Others...\n",
    "\n",
    "**In general, this is why we design our transformers to take string args as keys for callables rather than callables themselves!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "model_location = \"heart_disease_model.pkl\"\n",
    "with open(model_location, \"wb\") as mod:\n",
    "    joblib.dump(lgr_search.best_estimator_, mod, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    assert os.path.exists(model_location)\n",
    "    \n",
    "# demo how we can load and predict in one line!\n",
    "is_certain_class(joblib.load(model_location).predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a Jupyter \"magic function\" to see that the pkl file exists in the file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heart_disease_model.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls | grep \"heart_disease_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the REST API\n",
    "\n",
    "Once the Flask app is live, we can test its `predict` endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send me a valid POST! I accept JSON data only:\n",
      "\n",
      "\t{data=[...]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# if you have a proxy...\n",
    "os.environ['NO_PROXY'] = 'localhost'\n",
    "\n",
    "# test if it's running\n",
    "url = \"http://localhost:5000/predict\"\n",
    "\n",
    "# print the GET result\n",
    "response = requests.get(url)\n",
    "print(response.json()['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending data:\n",
    "\n",
    "Let's create a function that will accept a chunk of data, make it into a JSON and ship it to the REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid POST (n_samples=10)\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "def get_predictions(data, url, headers):\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # if data is a vector and not a matrix, we need a vec...\n",
    "    if len(data.shape) == 1:\n",
    "        data = np.asarray([data.tolist()])\n",
    "        \n",
    "    # make a JSON out of it\n",
    "    jdata = json.dumps({'data': data.tolist()})\n",
    "    response = requests.post(url, data=jdata, headers=headers).json()\n",
    "    print(response['message'])\n",
    "    return response['predictions']\n",
    "\n",
    "# ship last few for X_test\n",
    "print(get_predictions(X_test[-10:], url, headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
