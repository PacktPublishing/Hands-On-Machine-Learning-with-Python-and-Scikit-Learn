{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this notebook, we'll examine the role various scoring metrics have on our model performance/selection process*\n",
    "\n",
    "#### Central question: \"How do I know if my model performs well?\"\n",
    "\n",
    "### Selecting the proper metric\n",
    "\n",
    "Much of the ability for a model's efficacy to be effectively judged lies in the metric you select.\n",
    "\n",
    "* Is it tied to dollars saved?\n",
    "* How does it relate to decision making ability?\n",
    "* Is the class label imbalanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "__Example__: Boston housing\n",
    "\n",
    "* Two models\n",
    "* No model tuning\n",
    "* No pre-processing\n",
    "* No CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n",
    "                                                    test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression fits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "         n_estimators=50, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# first model\n",
    "reg_model1 = Lasso(random_state=random_state)\n",
    "reg_model1.fit(X_train, y_train)\n",
    "\n",
    "# second model\n",
    "reg_model2 = AdaBoostRegressor(random_state=random_state)\n",
    "reg_model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring interface:\n",
    "\n",
    "All scikit-learn scoring methods are in the `metrics` submodule:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import some_metric\n",
    "```\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "All scikit-learn scoring methods match the following signature:\n",
    "\n",
    "```python\n",
    "# interface:\n",
    "def some_metric(actual_target, predicted_target, *args, **kwargs):\n",
    "    ...\n",
    "    return float(some_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression model 1 MSE: 24.4298\n",
      "Regression model 2 MSE: 10.9930\n",
      "\n",
      "Regression model 1 R^2: 0.6669\n",
      "Regression model 2 R^2: 0.8501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Print results\n",
    "print(\"Regression model 1 MSE: %.4f\" \n",
    "      % mean_squared_error(y_test, \n",
    "                           reg_model1.predict(X_test)))\n",
    "\n",
    "# Print results\n",
    "print(\"Regression model 2 MSE: %.4f\\n\" \n",
    "      % mean_squared_error(y_test, \n",
    "                           reg_model2.predict(X_test)))\n",
    "\n",
    "print(\"Regression model 1 R^2: %.4f\" \n",
    "      % r2_score(y_test, reg_model1.predict(X_test)))\n",
    "\n",
    "print(\"Regression model 2 R^2: %.4f\" \n",
    "      % r2_score(y_test, reg_model2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Beware error terms vs. \"higher=better\" scoring terms!!!__\n",
    "\n",
    "Without any knowledge of what the metrics mean, we might select model 1 since it's MSE is higher if we don't understand the concept of *error*.\n",
    "\n",
    "Likewise, we might select model 1 on the basis of $R^{2}$ if we treat it like an error metric.\n",
    "\n",
    "__In reality, model 2 wins on the basis of both metrics!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Different challenges than regression:\n",
    "1. Consider false positives/negatives\n",
    "2. Consider very imbalanced events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative class labels: 976\n",
      "Positive class labels: 24\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=25, n_informative=13, \n",
    "                           n_redundant=2, n_repeated=0, n_classes=2, \n",
    "                           n_clusters_per_class=2, weights=[0.98], \n",
    "                           flip_y=0.01, class_sep=1.0, hypercube=True, \n",
    "                           shift=0.0, scale=1.0, shuffle=True, \n",
    "                           random_state=random_state)\n",
    "\n",
    "# need to stratify the split due to our imbalance!\n",
    "print(\"Negative class labels: %i\" % (y == 0).sum())\n",
    "print(\"Positive class labels: %i\" % (y == 1).sum())\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1 accuracy: 0.98000\n",
      "Clf 2 accuracy: 0.97500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# first model\n",
    "clf1 = LogisticRegression(random_state=random_state)\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "# second model\n",
    "clf2 = SGDClassifier(max_iter=25, random_state=random_state)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# print metrics\n",
    "print(\"Clf 1 accuracy: %.5f\" \n",
    "      % accuracy_score(y_test, clf1.predict(X_test)))\n",
    "\n",
    "print(\"Clf 2 accuracy: %.5f\" \n",
    "      % accuracy_score(y_test, clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which  model would you choose?\n",
    "\n",
    "Model 1 appears to out-perform model 2. But does that mean it's actually better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLF 1] Num '0' predictions: 199\n",
      "[CLF 1] Num '1' predictions: 1\n",
      "[CLF 1] Num FN: 4\n",
      "[CLF 1] Num FP: 0\n",
      "\n",
      "[CLF 2] Num '0' predictions: 200\n",
      "[CLF 2] Num '1' predictions: 0\n",
      "[CLF 2] Num FN: 5\n",
      "[CLF 2] Num FP: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prediction_report(model, model_name):\n",
    "    preds = model.predict(X_test)\n",
    "    print(\"[%s] Num '0' predictions: %i\" % (model_name, (preds == 0).sum()))\n",
    "    print(\"[%s] Num '1' predictions: %i\" % (model_name, (preds == 1).sum()))\n",
    "    \n",
    "    # more interesting...\n",
    "    fn = (y_test == 1) & (preds == 0)\n",
    "    fp = (y_test == 0) & (preds == 1)\n",
    "    print(\"[%s] Num FN: %i\" % (model_name, fn.sum()))\n",
    "    print(\"[%s] Num FP: %i\\n\" % (model_name, fp.sum()))\n",
    "\n",
    "prediction_report(clf1, \"CLF 1\")\n",
    "prediction_report(clf2, \"CLF 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Since the prior probabilities of a positive class label were 0.02, we basically got 98% accuracy by almost never predicting 1 in estimator 1!!!__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLF 1 report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       195\n",
      "          1       1.00      0.20      0.33         5\n",
      "\n",
      "avg / total       0.98      0.98      0.97       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"CLF 1 report:\")\n",
    "print(classification_report(y_test, clf1.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLF 2 report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       195\n",
      "          1       0.00      0.00      0.00         5\n",
      "\n",
      "avg / total       0.95      0.97      0.96       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py35/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"CLF 2 report:\")\n",
    "print(classification_report(y_test, clf2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### While the scope of this lesson is not to discuss class imbalance fixes, it *is* to introduce how confounding some scoring metrics may be\n",
    "\n",
    "__Recall__: The proportion of positives that are correctly identified as such (e.g. the percentage of sick people who are correctly identified as having the condition)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom scoring\n",
    "\n",
    "Perhaps you care about dollars saved. Your business partners provide you with the following table:\n",
    "\n",
    "* False positive = -$50\n",
    "\n",
    "* False negative = -$250\n",
    "\n",
    "* True positive = $2,500\n",
    "\n",
    "* True negative = $0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dollars saved with CLF 1: $1,500.0\n",
      "Dollars saved with CLF 2: $-1,250.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dollars_saved(actual, predicted):\n",
    "    fp = (actual == 0) & (predicted == 1)\n",
    "    fn = (actual == 1) & (predicted == 0)\n",
    "    tp = (actual == 1) & (predicted == 1)\n",
    "    tn = (actual == 0) & (predicted == 0)\n",
    "    \n",
    "    return np.sum([fp.sum() * -50.,\n",
    "                   fn.sum() * -250.,\n",
    "                   tp.sum() * 2500.])  # don't need TN, since it's worth nothing\n",
    "\n",
    "print(\"Dollars saved with CLF 1: ${:,}\".format(\n",
    "        dollars_saved(y_test, clf1.predict(X_test))))\n",
    "\n",
    "print(\"Dollars saved with CLF 2: ${:,}\".format(\n",
    "        dollars_saved(y_test, clf2.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(dollars_saved)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "custom_scorer = make_scorer(score_func=dollars_saved, greater_is_better=True)\n",
    "custom_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-aways\n",
    "\n",
    "* Know whether \"higher is better\" or not\n",
    "* Be able to explain intuitively what a score represents\n",
    "  - i.e., MSE means that the average squared residual term is X\n",
    "* Know whether your data makes a scoring metric useless (i.e., accuracy for imbalance problems)\n",
    "* Consider a custom scoring metric if it makes most sense for your problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
